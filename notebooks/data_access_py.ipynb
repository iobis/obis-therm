{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the OBIS thermal dataset from Python\n",
    "\n",
    "The `obistherm` dataset includes OBIS occurrence data matched with multiple sources of monthly temperature. Temperature data is extracted for each occurrence based on the date it was collected, at the recorded depth or across multiple depths. See how to download it here and how to use it here. The current version of `obistherm` is based on the OBIS full export of 2024-07-23 and covers the period of 1986 to 2024.\n",
    "\n",
    "## Accessing the dataset\n",
    "\n",
    "### Download a local copy\n",
    "\n",
    "The final dataset is available through the OBIS AWS S3 bucket `s3://obis-products/obistherm`. If you have the **AWS** CLI program installed in your computer, you can run the following in the command line:\n",
    "\n",
    "``` bash\n",
    "aws s3 cp --recursive s3://obis-products/obistherm . --no-sign-request\n",
    "```\n",
    "What will download all files to your local folder. Alternatively, on Python you can use the `boto3` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# AWS S3 configuration\n",
    "bucket_name = \"obis-products\"\n",
    "s3_folder = \"obistherm\"\n",
    "local_folder = \"obistherm\"\n",
    "\n",
    "# Create a local folder if it doesn't exist\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# List objects in the specified S3 folder\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder)\n",
    "s3_objects = response.get('Contents', [])\n",
    "\n",
    "# Download objects\n",
    "total = len(s3_objects)\n",
    "for i, obj in enumerate(s3_objects, start=1):\n",
    "    s3_key = obj['Key']\n",
    "    \n",
    "    # Skip folders (keys ending with \"/\")\n",
    "    if not s3_key.endswith('/'):\n",
    "        local_file = os.path.join(local_folder, os.path.relpath(s3_key, s3_folder))\n",
    "        \n",
    "        # Ensure the local directory exists\n",
    "        os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
    "        \n",
    "        # Download the object\n",
    "        print(f\"Downloading {i} out of {total}: {s3_key}\")\n",
    "        s3_client.download_file(bucket_name, s3_key, local_file)\n",
    "        print(f\"Downloaded: {s3_key} to {local_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded the data to a local folder, you can then open it using `pyarrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Important to add \"partitioning='hive'\" so that the year information is captured\n",
    "dataset = ds.dataset(local_folder, format=\"parquet\", partitioning=\"hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not open the data on memory, what enables you to work with such a big dataset (more than 103 million records) seamlessly on Python. The function `dataset` will open a representation of the dataset on Python, which you can later filter and do other operations. You can learn more about `arrow` [here](https://arrow.apache.org/docs/python/index.html) and on this [short tutorial](https://resources.obis.org/tutorials/arrow-obis/) (for R, but the principles are the same).\n",
    "\n",
    "You can quickly see all columns that are available on the dataset by running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AphiaID: int32\n",
      "species: string\n",
      "family: string\n",
      "id: string\n",
      "dataset_id: string\n",
      "occurrenceID: string\n",
      "datasetID: string\n",
      "minimumDepthInMeters: double\n",
      "maximumDepthInMeters: double\n",
      "decimalLongitude: double\n",
      "decimalLatitude: double\n",
      "eventDate: string\n",
      "date_mid: int64\n",
      "month: int32\n",
      "surfaceTemperature: double\n",
      "midTemperature: double\n",
      "deepTemperature: double\n",
      "bottomTemperature: double\n",
      "midDepth: double\n",
      "deepDepth: double\n",
      "minimumDepthTemperature: double\n",
      "maximumDepthTemperature: double\n",
      "minimumDepthClosestDepth: double\n",
      "maximumDepthClosestDepth: double\n",
      "coraltempSST: double\n",
      "murSST: double\n",
      "ostiaSST: double\n",
      "flag: int32\n",
      "geometry: binary\n",
      "h3_7: string\n",
      "year: int32\n",
      "-- schema metadata --\n",
      "r: 'A\n",
      "3\n",
      "262913\n",
      "197888\n",
      "5\n",
      "UTF-8\n",
      "531\n",
      "2\n",
      "531\n",
      "2\n",
      "16\n",
      "1\n",
      "262153\n",
      "8\n",
      "geometry\n",
      "781\n",
      "29\n",
      "N' + 1837\n",
      "geo: '{\"primary_column\":\"geometry\",\"columns\":{\"geometry\":{\"crs\":\"GEOGCRS[' + 1212\n"
     ]
    }
   ],
   "source": [
    "print(dataset.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing through the S3 storage\n",
    "\n",
    "Downloading a local copy is the best solution to speed up any operation you need to do. However, it is also possible to access the dataset directly from the S3 storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_storage = \"s3://obis-products/obistherm/\"\n",
    "ds_s3 = ds.dataset(s3_storage)\n",
    "print(ds_s3.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of any operation done with the S3 version will depend on your internet connection and the type of operation. **The dataset is organized by year (note that we use a [hive structure in the files](https://arrow.apache.org/docs/r/articles/dataset.html)), and any operation that filter the data for a single year will be faster, because Arrow will only need to read one file.**\n",
    "\n",
    "## Filtering and aggregating\n",
    "\n",
    "Once you opened the dataset, you can quickly generate summaries and filter the data. Let's start by looking at the number of records for the family Ocypodidae across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Filter directly on the dataset to keep operations on disk\n",
    "filtered_dataset = dataset.to_table(\n",
    "    filter=(\n",
    "        (ds.field(\"family\") == \"Ocypodidae\") &  # Filter where family is \"Ocypodidae\"\n",
    "        (~ds.field(\"species\").is_null())        # Filter out rows where species is NA/None\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert the filtered dataset to a Pandas DataFrame for further grouping\n",
    "df = filtered_dataset.to_pandas()\n",
    "\n",
    "# Perform grouping\n",
    "ocyp_recs = (\n",
    "    df.groupby([\"species\", \"year\"])\n",
    "    .size()  # Count occurrences\n",
    "    .reset_index(name=\"count\")  # Reset index and name the count column\n",
    ")\n",
    "\n",
    "print(ocyp_recs)\n",
    "\n",
    "#CPU times: user 22.8 s, sys: 5.15 s, total: 28 s\n",
    "#Wall time: 3.82 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how quick the operation is. What we have done:\n",
    "\n",
    "1. We start by filtering the data. In this case we filter by the family \"Ocypodidae\"  \n",
    "2. We filter to remove those with no \"species\" name, that would be not at the species rank  \n",
    "3. We convert this result to table, and then to `pandas`\n",
    "4. Then we group our data by _species_ and _year_  \n",
    "5. We then use `.size()` to count the number of records  \n",
    " \n",
    "Let's work with the 4 species with the largest number of records. We did the counts by year, so we will aggregate to get the total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ocypodidae Species:\n",
      "                    species  total\n",
      "24          Leptuca thayeri    260\n",
      "4           Austruca lactea    132\n",
      "79      Ucides occidentalis    123\n",
      "37  Ocypode ceratophthalmus     86\n"
     ]
    }
   ],
   "source": [
    "# Identify the top 4 species\n",
    "top_ocyp = (\n",
    "    ocyp_recs.groupby(\"species\")[\"count\"]  # Group by 'species' and sum 'count'\n",
    "    .sum()\n",
    "    .reset_index(name=\"total\")  \n",
    "    .sort_values(by=\"total\", ascending=False)  \n",
    "    .head(4) \n",
    ")\n",
    "\n",
    "print(top_ocyp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now filter the data for those species to check the temperatures across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_species_list = top_ocyp[\"species\"].tolist()\n",
    "\n",
    "top_ocyp_data = (\n",
    "    df[df[\"species\"].isin(top_species_list)]  \n",
    "    .loc[:, [ \n",
    "        \"species\", \"surfaceTemperature\", \"coraltempSST\", \n",
    "        \"murSST\", \"ostiaSST\", \"year\", \"month\", \n",
    "        \"decimalLongitude\", \"decimalLatitude\"\n",
    "    ]]\n",
    ")\n",
    "\n",
    "# Rename column\n",
    "top_ocyp_data = top_ocyp_data.rename(columns={\"surfaceTemperature\": \"glorysSST\"})\n",
    "\n",
    "print(top_ocyp_data.head())\n",
    "\n",
    "print(top_ocyp_data.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `.loc` to select only the columns that we are going to use. We also rename the surfaceTemperature column (which is the GLORYS product) to glorysSST.\n",
    "\n",
    "CoralTemp is the most complete product in this case, so we will focus on it. We can quickly produce a plot of temperature over time for those 4 species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import (\n",
    "    ggplot, aes, geom_point, facet_wrap, theme_light, geom_boxplot, geom_hline\n",
    ")\n",
    "\n",
    "# Add a 'date' column combining year and month\n",
    "top_ocyp_data[\"date\"] = pd.to_datetime(\n",
    "    top_ocyp_data[\"year\"].astype(str) + \"-\" +\n",
    "    top_ocyp_data[\"month\"].astype(str) + \"-01\"\n",
    ")\n",
    "\n",
    "(\n",
    "    ggplot(top_ocyp_data, aes(x=\"date\", y=\"coraltempSST\")) +\n",
    "    geom_point(aes(color=\"species\")) +\n",
    "    facet_wrap(\"~species\") +\n",
    "    theme_light()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a boxplot of the full data for each species. We will also get the .95 quantile, what we can use as an indication of thermal limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = (\n",
    "    top_ocyp_data.groupby(\"species\")[\"coraltempSST\"]\n",
    "    .quantile(0.95)\n",
    "    .reset_index(name=\"top_limit\")\n",
    ")\n",
    "\n",
    "(\n",
    "    ggplot(top_ocyp_data, aes(x=\"species\", y=\"coraltempSST\")) +\n",
    "    geom_boxplot(aes(fill=\"species\")) +\n",
    "    geom_hline(aes(yintercept=\"top_limit\", color=\"species\"), data=limits) +\n",
    "    theme_light()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that _Austruca lactea_ has the widest thermal range. Let's plot on a map the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(12, 8), \n",
    "    subplot_kw={'projection': ccrs.PlateCarree()}  # WGS84 projection\n",
    ")\n",
    "\n",
    "ax.add_feature(cfeature.LAND, facecolor='grey')\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(top_ocyp_data[\"decimalLongitude\"], top_ocyp_data[\"decimalLatitude\"])]\n",
    "top_ocyp_data_geo = gpd.GeoDataFrame(top_ocyp_data, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "top_ocyp_data_geo.plot(\n",
    "    ax=ax, \n",
    "    transform=ccrs.PlateCarree(), \n",
    "    marker=\"o\", \n",
    "    column=\"species\", \n",
    "    legend=True,\n",
    "    cmap=\"Set2\"\n",
    ")\n",
    "\n",
    "plt.title(\"Ocypodidae species records\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the README of the repository for more information about the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
